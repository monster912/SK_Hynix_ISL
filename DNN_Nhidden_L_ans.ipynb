{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\"> Convex optimization for machine learning: Part 2</div>\n",
    "#### <div style=\"text-align: right\"> Prof. Changho Suh, TA Gyeongjo Hwang, Doyeon Kim</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 심층신경망 (Deep Neural Network)\n",
    "\n",
    "- 이번 강의에서는 심층신경망을 구현하기위해 필요한 모든 함수를 구현할 것\n",
    "- 구현한 신경망을 이미지 분류문제에 적용해 볼 것\n",
    "\n",
    "**What you will learn to:**\n",
    "- 더 깊은 층(Layer)의 신경망을 구현(1 hidden layer보다 더 깊은 DNN)\n",
    "\n",
    "**Notation**:\n",
    "- 첨자 $[l]$는 $l^{th}$층의 값들을 의미\n",
    "    - 예) $a^{[L]}$는 $L^{th}$층의 활성함수 값, $W^{[L]}$과 $b^{[L]}$는 $L^{th}$층의 파라미터의 값을 의미\n",
    "- 첨자 $(i)$는 $i^{th}$번째 값을 의미\n",
    "    - 예) $x^{(i)}$는 $i^{th}$번째 training example를 의미\n",
    "- 아래첨자 $i$는 벡터의 $i^{th}$ 번째 요소를 의미\n",
    "    - 예) $a^{[l]}_i$는 $l^{th}$ 층의 활성함수 값의 $i^{th}$번째 요소를 의미\n",
    "    - 예) $a^{[l](i)}_j$는 $i^{th}$번째 training example의 $l^{th}$ 층의 활성함수 값의 $j^{th}$번째 요소를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from NL_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  개요\n",
    "\n",
    "더욱 깊은 Neural network를 구현하기 위한 instruction은 다음과 같다.\n",
    "\n",
    "- 2-layer neural network에 대하여 parameter initialization 및 $L$-layer neural network에 대하여 initialization 구현\n",
    "- forward pass 모듈 구현 (figure.1 의 보라색 블록 참조).\n",
    "     - 각 층의 linear 연산구현($Z^{[l]}$).\n",
    "     - 활성함수(activation fuction)는 주어질 것 (ReLU/sigmoid).\n",
    "     - 위의 두 연산을 [LINEAR->ACTIVATION] forward 연산으로 합침\n",
    "     - 위의 [LINEAR->RELU] forward function을 L-1번 쌓는다. (for layers 1 through L-1) 그리고 마지막단에는  [LINEAR->SIGMOID] forward function을 쌓는다.(for the final layer $L$).\n",
    "- 손실함수 계산\n",
    "- backpropagation 모듈 구현 (figure.1 의 적색 블록 참조).\n",
    "    - Linear 연산의 backpropagation 구현\n",
    "    - 활성함수(activation function)의 gradient 값은 주어질 것 (relu_backward/sigmoid_backward) \n",
    "    - 위의 두 연산을 [LINEAR->ACTIVATION] backward 연산으로 합침\n",
    "    - 위의 [LINEAR->RELU] backward function을 L-1번 쌓는다. 그리고 마지막 단에 [LINEAR->SIGMOID] backward function을 쌓는다.\n",
    "- 마지막으로 parameter를 업데이트 한다,\n",
    "\n",
    "<img src=\"figures/final outline.png\" style=\"width:800px;\">\n",
    "<caption><center> Figure 1</center></caption><br>\n",
    "\n",
    "\n",
    "**Note** 모든 forward() 함수는 그에 대응하는 backward() 함수가 존재하며 이는 깊은 신경망에서 gradient 계산을 쉽게 구현하기 위함이다. 이 강의에서 역시 cache를 사용하여 gradient 계산에 이용할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "### L-layer Neural Network\n",
    "\n",
    "<table style=\"width:90%\">\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> Shape of W </td> \n",
    "        <td> Shape of b  </td> \n",
    "        <td> Activation </td>\n",
    "        <td> Shape of Activation </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> Layer 1 </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> Layer 2 </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "   <tr>\n",
    "        <td> Layer L-1 </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    \n",
    "   <tr>\n",
    "        <td> Layer L </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "파이썬 $W X + b$연산은 브로드캐스팅을 지원한다:\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습 :** L-layer Neural Network의 initialization 함수를 구현하시오.\n",
    "\n",
    "**Instructions**:\n",
    "- 모델구조: *[LINEAR -> RELU] $ \\times$ (L-1) -> [LINEAR -> SIGMOID]*.\n",
    "- `np.random.rand(shape) * 0.01`를 이용하여 W 초기화.\n",
    "- `np.zeros(shape)`를 이용하여 bias 초기화.\n",
    "- 모델 층의 모든 유닛 개수를 저장하고 있는 layer_dim리스트 값을 인풋으로 받는다.\n",
    "\n",
    "예) $L=1$에 대한 파라미터 초기화: \n",
    "```python\n",
    "if L == 1:\n",
    "    parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "    parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
      " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
      " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
      " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
      " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
      " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
    " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
    " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
    " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]<br>\n",
    "b1 = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]<br>\n",
    "W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
    " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
    " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]<br>\n",
    "b2 = [[0.]\n",
    " [0.]\n",
    " [0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation module\n",
    "\n",
    "### Linear Forward \n",
    "다음 세개의 연산을 차례로 구현할 것\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (whole model)\n",
    "\n",
    "Linear forward연산은 다음과 같다(Vectorized version):\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "where $A^{[0]} = X$. \n",
    "\n",
    "**실습 :** Linear foward를 구현하시오.\n",
    "\n",
    "**Tip**:\n",
    "\n",
    "$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$ 계산시 `np.dot()`가 유용. Dimension 체크는`W.shape`이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W, A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z = [[ 3.26295337 -1.23429987]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear-Activation Forward\n",
    "\n",
    "다음 두 활성함수를 사용:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. \n",
    "\n",
    "- **ReLU**: $A = RELU(Z) = max(0, Z)$. \n",
    "\n",
    "`sigmoid` 및 `relu` function는 제공될 것이고 다음 **2개**의 값을 리턴한다:\n",
    "\n",
    "the activation value \"`a`\" , a \"`cache`\"\n",
    "\n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```\n",
    "\n",
    "**실습 :**  *LINEAR->ACTIVATION* layer를 구현하시오. \n",
    "\n",
    "**Note :**Mathematical relation: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ where the activation \"g\" can be sigmoid() or relu()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sigmoid: A = [[0.96890023 0.11013289]]\n",
    "With ReLU: A = [[3.43896131 0.        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-Layer Model \n",
    "\n",
    "L-layer model을 구현하기 위해서는 앞서 구현한 linear->activation 연산[`linear_activation_forward` with RELU]를 $L-1$번, [`linear_activation_forward` with SIGMOID]를 한 번 쌓아야 한다.\n",
    "\n",
    "<img src=\"figures/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> Figure 2 : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>\n",
    "\n",
    "**Exercise**: 위 figure2 모델의 forward연산을 구현하시오.\n",
    "\n",
    "**Instruction**: 파이썬 변수 `AL`는 $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$를 나타냄 (=`Yhat`, i.e., $\\hat{Y}$.) \n",
    "\n",
    "**Tips**:\n",
    "- 앞서 구현한 forward모듈을 사용하시오 \n",
    "- for loop를 사용하시오 ([LINEAR->RELU] 연산 (L-1) 번)\n",
    "- `list.append(c)`메소드를 이용하여 cache값을 저장하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, \n",
    "                                             parameters[\"W\" + str(l)], \n",
    "                                             parameters[\"b\" + str(l)], \n",
    "                                             activation='relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, \n",
    "                                             parameters[\"W\" + str(L)], \n",
    "                                             parameters[\"b\" + str(L)], \n",
    "                                             activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.17007265 0.2524272 ]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AL = [[0.17007265 0.2524272 ]]\n",
    "Length of caches list = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function 구현\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = (-1./ m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1-Y), np.log( 1-AL)))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost = 0.41493159961539694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation module\n",
    "\n",
    "\n",
    "**Reminder**: \n",
    "<img src=\"figures/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> Figure 3 : Forward and Backward propagation for LINEAR->RELU->LINEAR->SIGMOID<br> </center></caption>\n",
    "\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (whole model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear backward\n",
    "\n",
    "linear part: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "$dZ^{[l]} = \\dfrac{\\partial J }{\\partial Z^{[l]}}$ 값만 계산되고 나면 $(dW^{[l]}, db^{[l]} dA^{[l-1]})$ 값들을 차례로 계산할 수 있음\n",
    "\n",
    "<img src=\"figures/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> Figure 4 </center></caption>\n",
    "\n",
    "$dZ^{[l]}$값을 이용하여 세 값 $(dW^{[l]}, db^{[l]}, dA^{[l]})$ 을 계산:\n",
    "$$ dW^{[l]} = \\dfrac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\dfrac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\dfrac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
    "\n",
    "**실습 :**  위 식을 참고하여 linear_backward()를 구현하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = np.dot(dZ, cache[0].T) \n",
    "    db = np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.2015379   2.81370193  3.2998501 ]]\n",
      "db = [[1.01258895]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dA_prev = [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]]<br>\n",
    "dW = [[-0.2015379   2.81370193  3.2998501 ]]<br>\n",
    "db = [[1.01258895]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation backward\n",
    "\n",
    " **`linear_backward`** 를 이용하여 **`linear_activation_backward`** 를 구현 \n",
    "\n",
    "$g(.)$가 활성함수(activation function)일 때,\n",
    "`sigmoid_backward` and `relu_backward`는 $dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$를 계산  \n",
    "\n",
    "**Exercise**:  *LINEAR->ACTIVATION* layer 의backpropagation를 구현하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.20533573  0.19557101 -0.03936168]]\n",
      "db = [[-0.11459244]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.89027649  0.74742835 -0.20957978]]\n",
      "db = [[-0.41675785]]\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid:<br>\n",
    "dA_prev = [[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]]<br>\n",
    "dW = [[ 0.20533573  0.19557101 -0.03936168]]<br>\n",
    "db = [[-0.11459244]]<br>\n",
    "\n",
    "relu:<br>\n",
    "dA_prev = [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]]<br>\n",
    "dW = [[ 0.89027649  0.74742835 -0.20957978]]<br>\n",
    "db = [[-0.41675785]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-Model Backward \n",
    "\n",
    "앞서 `L_model_forward` function을 구현한 것과 같은 방식으로 `L_model_backward` function을 구현해 볼 것이다.\n",
    "\n",
    "\n",
    "<img src=\"figures/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  Figure 5 : Backward pass  </center></caption>\n",
    "\n",
    "`dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$를 계산하기 위해서는\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```\n",
    "를 이용\n",
    "\n",
    "L_model_forward에서 cache에 저장된 (X,W,b, and z)값들과 for loop를 이용하여 다음과 같이 구현:\n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}$$\n",
    "\n",
    "ex) $l=3$층의 파라미터 $dW^{[l]}$는 `grads[\"dW3\"]`안에서 retrieve\n",
    "\n",
    "**실습 :** *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID*  모델의 backpropagation 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = (1 / m) * ( - np.divide(Y, AL) + np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.          0.26128951]\n",
      " [ 0.         -0.1634603 ]\n",
      " [ 0.         -0.16035202]\n",
      " [ 0.         -0.37039594]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
    " [0.         0.         0.         0.        ]\n",
    " [0.05283652 0.01005865 0.01777766 0.0135308 ]]<br>\n",
    "db1 = [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]]<br>\n",
    "dA1 = [[ 0.          0.26128951]\n",
    " [ 0.         -0.1634603 ]\n",
    " [ 0.         -0.16035202]\n",
    " [ 0.         -0.37039594]]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Update Parameters\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "**실습 :** 파라미터 업데이트를 위한 `update_parameters()`함수를 구현하시오\n",
    "\n",
    "**Instructions**:\n",
    "모든 $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$ 에 대하여 업데이트 (for loop 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]]<br>\n",
    "b1 = [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]]<br>\n",
    "W2 = [[-0.55569196  0.0354055   1.32964895]]<br>\n",
    "b2 = [[-0.84610769]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. L-layer Neural Network\n",
    "\n",
    "**7-1.하나의 L_layer_model 함수로 구현해보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  5-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate=learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7-2. Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209)\n",
      "Cost after iteration 0: 0.771749\n",
      "Cost after iteration 100: 0.672053\n",
      "Cost after iteration 200: 0.648263\n",
      "Cost after iteration 300: 0.611507\n",
      "Cost after iteration 400: 0.567047\n",
      "Cost after iteration 500: 0.540138\n",
      "Cost after iteration 600: 0.527930\n",
      "Cost after iteration 700: 0.465477\n",
      "Cost after iteration 800: 0.369126\n",
      "Cost after iteration 900: 0.391747\n",
      "Cost after iteration 1000: 0.315187\n",
      "Cost after iteration 1100: 0.272700\n",
      "Cost after iteration 1200: 0.237419\n",
      "Cost after iteration 1300: 0.199601\n",
      "Cost after iteration 1400: 0.189263\n",
      "Cost after iteration 1500: 0.161189\n",
      "Cost after iteration 1600: 0.148214\n",
      "Cost after iteration 1700: 0.137775\n",
      "Cost after iteration 1800: 0.129740\n",
      "Cost after iteration 1900: 0.121225\n",
      "Cost after iteration 2000: 0.113821\n",
      "Cost after iteration 2100: 0.107839\n",
      "Cost after iteration 2200: 0.102855\n",
      "Cost after iteration 2300: 0.100897\n",
      "Cost after iteration 2400: 0.092878\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (cat/non-cat) 데이터셋 로드\n",
    "from RL_utils import load_dataset\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "X_train = X_train_flatten / 255.\n",
    "X_test = X_test_flatten / 255.\n",
    "Y_train = Y_train_orig.reshape(1, -1)\n",
    "Y_test = Y_test_orig.reshape(1, -1)\n",
    "\n",
    "print(X_train.shape)\n",
    "parameters = L_layer_model(X_train, Y_train, layers_dims, num_iterations=2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy of the trained model on train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 98.565%\n",
      "Test acc: 80.000%\n"
     ]
    }
   ],
   "source": [
    "yhat_train, _ = L_model_forward(X=X_train, parameters=parameters)\n",
    "yhat_test, _ = L_model_forward(X=X_test, parameters=parameters)\n",
    "\n",
    "y_pred_train = (yhat_train > 0.5)\n",
    "y_pred_test = (yhat_test > 0.5)\n",
    "\n",
    "print(\"Train acc: {:.3%}\".format(np.sum(y_pred_train == Y_train) / Y_train.shape[1]))\n",
    "print(\"Test acc: {:.3%}\".format(np.sum(y_pred_test == Y_test) / Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
